{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c87b1a-22db-4ef2-a1b2-f65751f82168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.weight_init import uniform_init, xavier_init\n",
    "from mmdet.apis import init_detector\n",
    "\n",
    "\n",
    "recog_config = \"crnn_academic_dataset.py\"\n",
    "recog_ckpt = \"crnn_academic-a723a1c5.pth\"\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "config = mmcv.Config.fromfile(recog_config)\n",
    "model_config = config.model\n",
    "\n",
    "max_seq_len=40\n",
    "\n",
    "recog_model = init_detector(\n",
    "    recog_config, recog_ckpt, device=device)\n",
    "if hasattr(recog_model, 'module'):\n",
    "    recog_model = recog_model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e432499-0d8d-4f29-8f23-d0612e4b5bb0",
   "metadata": {},
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fa4cd5-cd82-471b-8ab9-9e3c85d8178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(pil_im, dst_height=32, dst_min_width=32, width_downsample_ratio=1.0 / 16):\n",
    "\n",
    "    (ori_width, ori_height) = pil_im.size\n",
    "    new_width = math.ceil(float(dst_height) / ori_height * ori_width)\n",
    "    width_divisor = int(1 / width_downsample_ratio)\n",
    "\n",
    "    if new_width % width_divisor != 0:\n",
    "        new_width = round(new_width / width_divisor) * width_divisor\n",
    "    \n",
    "    new_width = max(dst_min_width, new_width)\n",
    "    \n",
    "    im_resized = pil_im.resize(size=(new_width, dst_height))\n",
    "    \n",
    "    return np.asarray(im_resized, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad6f200-1f13-402b-b4b4-7aed5f8b2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"test_images/image_2.jpg\"\n",
    "\n",
    "pil_im = Image.open(image).convert('L')\n",
    "im_array = preprocess_image(pil_im)\n",
    "\n",
    "im_array = (im_array-127.0)/127.0\n",
    "\n",
    "image_tensor = torch.from_numpy(im_array)\n",
    "image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "data = dict(img=image_tensor.unsqueeze(0), img_metas=[{'filename': 'test_images/image_1.jpg', 'resize_shape': (32, 48), 'valid_ratio': 1.0}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b65907-f446-47db-8bc8-2c0bdbfd83de",
   "metadata": {},
   "source": [
    "# 1. Quantize backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5486796-d657-48e4-ae19-c54d89a35d61",
   "metadata": {},
   "source": [
    "## 1.1 Imports and model defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880057d7-a928-488c-925a-11c315e407c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import VeryDeepVgg\n",
    "\n",
    "class VeryDeepVgg(nn.Module):\n",
    "    \"\"\"Implement VGG-VeryDeep backbone for text recognition, modified from\n",
    "      `VGG-VeryDeep <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        leaky_relu (bool): Use leakyRelu or not.\n",
    "        input_channels (int): Number of channels of input image tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, leaky_relu=True, input_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        self.channels = nm\n",
    "\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv_relu(i, batch_normalization=False):\n",
    "            n_in = input_channels if i == 0 else nm[i - 1]\n",
    "            n_out = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(n_in, n_out, ks[i], ss[i], ps[i]))\n",
    "            if batch_normalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(n_out))\n",
    "            if leaky_relu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        conv_relu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        conv_relu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        conv_relu(2, True)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        conv_relu(4, True)\n",
    "        conv_relu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        conv_relu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                xavier_init(m)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                uniform_init(m)\n",
    "\n",
    "    def out_channels(self):\n",
    "        return self.channels[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output = self.cnn(x)\n",
    "\n",
    "        x = self.quant(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "backbone_cfg = model_config['backbone'].copy()\n",
    "backbone_cfg.pop('type')\n",
    "\n",
    "fuser_list = [\n",
    "    [\"cnn.conv0\", \"cnn.relu0\"],\n",
    "    [\"cnn.conv1\", \"cnn.relu1\"],\n",
    "    [\"cnn.conv2\", \"cnn.batchnorm2\", \"cnn.relu2\"],\n",
    "    [\"cnn.conv3\", \"cnn.relu3\"],\n",
    "    [\"cnn.conv4\", \"cnn.batchnorm4\", \"cnn.relu4\"],\n",
    "    [\"cnn.conv5\", \"cnn.relu5\"],\n",
    "    [\"cnn.conv6\", \"cnn.batchnorm6\", \"cnn.relu6\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf043b6f-8a86-40b8-b92a-c5b6ae122b5b",
   "metadata": {},
   "source": [
    "## 1.2 load model initialized with pretrained weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbe1a95-39d0-4393-9d1c-fbc8b143ff08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = VeryDeepVgg(**backbone_cfg)\n",
    "backbone.load_state_dict(recog_model.backbone.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fe55f-84a1-489c-b8ee-cad0920c70b5",
   "metadata": {},
   "source": [
    "## 1.3 run static quantization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdde4f3d-5a0b-4ed4-9102-f9f150868e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "backbone.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Fuse the activations to preceding layers, where applicable.\n",
    "# This needs to be done manually depending on the model architecture.\n",
    "# Common fusions include `conv + relu` and `conv + batchnorm + relu`\n",
    "backbone_fused = torch.quantization.fuse_modules(backbone, fuser_list)\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "backbone_prepared = torch.quantization.prepare(backbone_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = data[\"img\"]\n",
    "backbone_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "backbone_quantized = torch.quantization.convert(backbone_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1693e4-534e-4172-8efb-59fd423e25c4",
   "metadata": {},
   "source": [
    "# 2. Quantize Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2278606-c569-4057-a0b3-0a61fdb0636e",
   "metadata": {},
   "source": [
    "## 2.1 Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45467305-9521-46e1-a127-7351ab9f4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmocr.models.textrecog import CRNNDecoder, CTCConvertor\n",
    "\n",
    "label_converter_cfg = model_config['label_convertor'].copy()\n",
    "label_converter_cfg.pop('type')\n",
    "label_converter_cfg.update(max_seq_len=max_seq_len)\n",
    "label_converter = CTCConvertor(**label_converter_cfg)\n",
    "\n",
    "\n",
    "decoder_cfg = model_config['decoder'].copy()\n",
    "decoder_cfg.pop('type')\n",
    "decoder_cfg.update(num_classes=label_converter.num_classes())\n",
    "decoder_cfg.update(start_idx=label_converter.start_idx)\n",
    "decoder_cfg.update(padding_idx=label_converter.padding_idx)\n",
    "decoder_cfg.update(max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6d1e4-c9a3-40e9-b37a-edb605c47997",
   "metadata": {},
   "source": [
    "## 2.2 load model initialized with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b921601-c60b-4727-9232-129a64ad5c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = CRNNDecoder(**decoder_cfg)\n",
    "decoder.load_state_dict(recog_model.decoder.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ab8e8-0eb6-4dbd-91d6-f7edc57456b1",
   "metadata": {},
   "source": [
    "## 2.3 run dynamic quantization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5467ac5e-08b1-4407-b99d-afe2d7f36701",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_decoder = torch.quantization.quantize_dynamic(decoder, {nn.LSTM, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea2305-3188-49b7-aaa1-d5d16802c017",
   "metadata": {},
   "source": [
    "# 3. Reassemble CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7878918-d5fb-414e-99a5-b4f465001279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmocr.models.textrecog import CTCLoss\n",
    "from utils import CRNNNet\n",
    "\n",
    "loss_cfg = model_config['decoder'].copy()\n",
    "loss_cfg.pop('type')\n",
    "loss_cfg.update(ignore_index=label_converter.padding_idx)\n",
    "loss = CTCLoss(**loss_cfg)\n",
    "\n",
    "model = CRNNNet(preprocessor=None,\n",
    "            backbone=backbone_quantized,\n",
    "            decoder=quantized_decoder,\n",
    "            loss=loss,\n",
    "            label_convertor=label_converter,\n",
    "            pretrained=None,\n",
    "            test_cfg=config.get('test_cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce48712-d838-4702-b8ea-5076f75334aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    det_result = model(return_loss=False, rescale=True, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d2e85f-955a-4523-a3b1-d3fc12b7f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '29', 'score': [0.7538787126541138, 0.5606470704078674]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593458ad-a513-4397-a0e5-e1409861b94c",
   "metadata": {},
   "source": [
    "# 4. Optimize for Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d2190ced-fd07-41e1-b3a9-1a44383006ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "from typing import List, Dict\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, inputs: List[torch.Tensor]):\n",
    "        x = inputs[0].unsqueeze(0)\n",
    "        data = dict(img=x, img_metas=[{'filename': 'image_1.jpg', 'resize_shape': (32, 48), 'valid_ratio': 1.0}])\n",
    "\n",
    "        out = self.model(return_loss=False, rescale=True, **data)\n",
    "\n",
    "        text_val = out[0]['text']\n",
    "        text_ascii_int = []\n",
    "        for v in text_val:\n",
    "            text_ascii_int.append(ord(v))\n",
    "\n",
    "        return torch.IntTensor(text_ascii_int), torch.FloatTensor(out[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d6eeb8f0-6036-47a0-96ce-355558fec125",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.rand(1, 1, 32, 48)\n",
    "wrapped_model = Wrapper(model)\n",
    "traced_script_module = torch.jit.trace(wrapped_model, example)\n",
    "traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n",
    "traced_script_module_optimized._save_for_lite_interpreter(\"crcnn.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "795e24e6-1e0f-400c-88d0-f166dcf7b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ascii, text_scores = wrapped_model([image_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4254b127-c21c-42b5-ac69-5f7f546e0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ascii = text_ascii.numpy().tolist()\n",
    "text_scores = text_scores.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7257ede2-35ae-4974-92b9-4b8f741dae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '9']\n",
      "[0.7538787126541138, 0.5606470704078674]\n"
     ]
    }
   ],
   "source": [
    "text = [chr(x) for x in text_ascii]\n",
    "\n",
    "print(text)\n",
    "print(text_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ca043-d9ab-4a57-8802-51536d8bc675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
